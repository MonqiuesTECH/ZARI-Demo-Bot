# -*- coding: utf-8 -*-
"""ZARI Bot Voice

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wK4cSzINCbYENCTRP4y1-KTS38T0VQJo
"""
pdf_path = "dummy.pdf"
from pdf2image import convert_from_path
import pytesseract
import tempfile
import os

def extract_text_with_ocr(pdf_path):
    with tempfile.TemporaryDirectory() as temp_dir:
        images = convert_from_path(pdf_path)
        text = ""
        for i, image in enumerate(images):
            image_path = os.path.join(temp_dir, f"page_{i}.png")
            image.save(image_path, "PNG")
            page_text = pytesseract.image_to_string(image)
            text += page_text + "\n\n"
    return text

ocr_text = extract_text_with_ocr(pdf_path)
print(ocr_text[:1000])  # preview the first 1000 characters

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# Split the OCR text into paragraph-like chunks
chunks = ocr_text.split("\n\n")

# Create embeddings
embedder = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = embedder.encode(chunks)

# Build a FAISS index for similarity search
index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(np.array(embeddings))

print(f"Embedded {len(chunks)} chunks")

from transformers import pipeline

# Load a small model for CPU-safe text generation
from transformers import pipeline
qa_pipeline = pipeline("text2text-generation", model="google/flan-t5-small", device=-1)

def ask_zari(question):
    context = "\n".join(get_relevant_chunks(question))
    context = context[:3000]  # optional: allow more context if needed

    prompt = f"""
You are ZARI Agent, a helpful and direct automation strategist.

Use the context below to answer the question clearly, in a way a founder would understand.

Context:
{context}

Question:
{question}

Answer:"""

    result = qa_pipeline(prompt, max_new_tokens=300)[0]["generated_text"]
    return result.strip()

# Helper function to retrieve top-k relevant chunks
def get_relevant_chunks(question, top_k=1):
    question_embedding = embedder.encode([question])
    distances, indices = index.search(np.array(question_embedding), top_k)
    return [chunks[i] for i in indices[0]]

print(ask_zari("What does ZARI do in month 1?"))

def ask_zari(question):
    print("‚úÖ Received question:", question)
    try:
        context = "\n".join(get_relevant_chunks(question))
        print("üìö Context extracted (length):", len(context))
        context = context[:3000]  # optional: limit to avoid overflow

        prompt = f"""
You are ZARI Agent, a helpful and direct automation strategist.

Use the context below to answer the question clearly, in a way a founder would understand.

Context:
{context}

Question: {question}
Answer:"""

        print("‚úâÔ∏è Prompt sent to model:\n", prompt)

        result = qa_pipeline(prompt, max_new_tokens=300)[0]["generated_text"]
        print("ü§ñ Model response:", result)
        return result.strip()

    except Exception as e:
        print("‚ùå Error inside ask_zari:", str(e))
        return "Sorry, something went wrong while processing your request."

import gradio as gr

def chat_with_zari(question):
    return ask_zari(question)

gr.Interface(
    fn=chat_with_zari,
    inputs=gr.Textbox(lines=2, placeholder="Ask ZARI Agent anything..."),
    outputs="text",
    title="ZARI Agent",
    description="Ask ZARI Agent questions about your business, ops, or automation strategy."
).launch(share=True, debug=True)
